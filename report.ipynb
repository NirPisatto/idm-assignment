{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDM:Assignment\n",
    "\n",
    "## News Classification based on Their Head lines\n",
    "\n",
    "### ChanpisethChap\n",
    "\n",
    "Royal University of PhnomPenh\n",
    "\n",
    "Group: MISA Pisatto, Hok Lenghak, Yorn Chanvisal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "### 1.1 Scraping News Articles\n",
    "\n",
    "In the initial phase of the data collection process, the script employs Selenium and the Chrome WebDriver to systematically scrape news articles from the CNN website. The retrieved data is subsequently stored in a CSV file named 'data_set.csv'. To ensure data integrity, the script first checks for the existence of the CSV file; if absent, it creates the file with a header. The Chrome WebDriver is configured to navigate to a specific webpage, namely, 'https://www.nbcnews.com/archive/articles/2023/december', where news articles are archived. Following this, the script identifies the main HTML tag ('MonthPage') and extracts relevant information from anchor tags ('a') assumed to contain article titles. Each title, along with its corresponding ID and URL, is appended to a list for further processing.\n",
    "\n",
    "Subsequently, the script iterates through the collected titles, visiting each article's URL to extract additional details such as category, timestamp, author, and hostname. To maintain transparency and diagnose potential issues, the script prints the collected data and logs it into a file named 'logs.log'. Any encountered errors during the scraping process are caught, printed, and logged, ensuring a comprehensive record of the data collection process. Finally, after completing the scraping and data appending procedures, the Chrome WebDriver is gracefully closed, and a confirmation message is displayed, indicating the successful addition of data to the 'data_set.csv' file.\n",
    "\n",
    "We eexecuted this process a total of 12 times, spanning a duration of 12 months, with the objective of accumulating an extensive dataset of over 10,000 rows of news articles.\n",
    "\n",
    "After completing the aforementioned process, the resulting dataset includes the following entry:\n",
    "\n",
    "```\n",
    "'ID', 'TITLE', 'URL', 'Source', 'CATEGORY', 'Key', 'Website', 'Timestamp'\n",
    "1, \"Michael Latt, Hollywood social justice advocate, fatally shot in his home by an intruder, police say\", https://www.nbcnews.com/news/us-news/hollywood-social-justice-advocate-killed-home-intruder-rcna127406, U.S. NEWS, 2023-12-01T00:25:52.224Z, By Natalie Kainz and Phil Helsel, www.nbcnews.com\n",
    "```\n",
    "\n",
    "This entry signifies a news article titled \"Michael Latt, Hollywood social justice advocate, fatally shot in his home by an intruder, police say,\" categorized under U.S. NEWS. The article was published on December 1, 2023, at 00:25:52.224 UTC. It was authored by Natalie Kainz and Phil Helsel and is accessible via the URL: [https://www.nbcnews.com/news/us-news/hollywood-social-justice-advocate-killed-home-intruder-rcna127406](https://www.nbcnews.com/news/us-news/hollywood-social-justice-advocate-killed-home-intruder-rcna127406)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Declare and use logger incase app crashes scv won't be created\n",
    "logging.basicConfig(filename='logs.log', level=logging.INFO, format='%(asctime)s [%(levelname)s]: %(message)s')\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = 'data_set.csv'\n",
    "\n",
    "# Check if the file exists, if not create it with header\n",
    "file_exists = False\n",
    "try:\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        if any(reader):\n",
    "            file_exists = True\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set up the web driver (you need to have the appropriate web driver executable installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the NBC news 2023 december archive\n",
    "driver.get(\"https://www.nbcnews.com/archive/articles/2023/december\")\n",
    "\n",
    "# Get all the titles from the page\n",
    "main_tag = driver.find_element(By.CLASS_NAME, 'MonthPage')\n",
    "title_tags = main_tag.find_elements(By.TAG_NAME, 'a')\n",
    "titles = []\n",
    "index = 0\n",
    "\n",
    "# Loop through the titles and append them to the titles list\n",
    "for title in title_tags:\n",
    "    index += 1\n",
    "    title = {\"id\": index, \"title\": title.text, \"url\": title.get_attribute('href')}\n",
    "    titles.append(title)\n",
    "\n",
    "# Append data to the CSV file\n",
    "with open(csv_file_path, 'a', newline='') as file:\n",
    "\n",
    "    # Loop through the titles and get the category, timestamp, author and hostname\n",
    "    for title in titles:\n",
    "        try:\n",
    "            driver.get(title.get('url'))\n",
    "            span_tag = driver.find_element(By.CSS_SELECTOR,  'span[data-testid=\"unibrow-text\"]')\n",
    "            data_tag = driver.find_element(By.CSS_SELECTOR,  'time[data-testid=\"timestamp__datePublished\"]')\n",
    "            div_tag = driver.find_element(By.CSS_SELECTOR,  'div[data-activity-map=\"inline-byline-article-top\"]')\n",
    "            title['category'] = span_tag.text\n",
    "            title['timestamp'] = data_tag.get_attribute('content')\n",
    "            title['author'] = div_tag.text\n",
    "            title['hostname'] = 'www.nbcnews.com'\n",
    "\n",
    "            fieldnames = title.keys()\n",
    "            logging.info(f\"Appended to {csv_file_path}: {title}\")\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writerow(title)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print(f\"Data has been appended to {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "### 2.1 Loading Data\n",
    "- The script then loads the collected data from the CSV file using the pandas library.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data set file path\n",
    "src_file = 'newsCorpora.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "dataframe = pd.read_csv(src_file, encoding=\"utf8\",quotechar=\"\\\"\", engine='python', usecols=[\"TITLE\", \"CATEGORY\"])\n",
    "# dataframe = pd.read_csv(src_file, sep='\\t', header=None, names=['ID', 'TITLE', 'URL', 'Source', 'CATEGORY', 'Key', 'Website', 'Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Filtering\n",
    "- The dataset is filtered based on the number of occurrences of each category.\n",
    "- Categories with counts less than or equal to 10 are excluded from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the data set to only the categories with more than 10 data points\n",
    "category_count = dataframe.groupby(\"CATEGORY\").size().reset_index(name='COUNT')\n",
    "category_count = category_count[category_count[\"COUNT\"] > 50]\n",
    "dataframe = dataframe[dataframe[\"CATEGORY\"].isin(category_count[\"CATEGORY\"])]\n",
    "\n",
    "\n",
    "# Remove the categories that are not needed\n",
    "dataframe = dataframe[~dataframe[\"CATEGORY\"].isin([\"U.S. NEWS\", \"NEWS\", \"WORLD\"])]\n",
    "\n",
    "\n",
    "print(dataframe.groupby(\"CATEGORY\").size().reset_index(name='COUNT'))\n",
    "print(dataframe.head())\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Handling Missing Data\n",
    "- The script checks for missing data and reports if any are found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing data\n",
    "if(any(dataframe.isnull().any())):\n",
    "    print('Missing Data\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "else:\n",
    "    print('NO missing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Handling Duplicate Data\n",
    "- Duplicate rows are identified and removed from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate\n",
    "if(any(dataframe.duplicated())==True):\n",
    "    print('Duplicate rows found')\n",
    "    print('Number of duplicate rows= ', dataframe[dataframe.duplicated()].shape[0])\n",
    "    dataframe.drop_duplicates(inplace=True,keep='first')\n",
    "    dataframe.reset_index(inplace=True,drop=True)\n",
    "    print('Dropping duplicates\\n')\n",
    "else:\n",
    "    print('NO duplicate data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Text Cleaning and Tokenization\n",
    "- NLTK libraries are used to download necessary resources (stopwords, punkt, wordnet).\n",
    "- A custom tokenizer function is defined to lowercase text, remove digits, punctuation, and stopwords, and lemmatize the remaining tokens.\n",
    "- A pipeline is created to perform Count Vectorization and TF-IDF transformation on the 'TITLE' column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Set the sklearn pipeline to return pandas dataframe\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Function for cleaning and tokenize the headline\n",
    "def tokenize(doc):\n",
    "  document = doc.lower() # convert the content of the headline to lowercase\n",
    "  document = re.sub(r'\\d+', '', document) # remove all of the digits inside of the content (using regular expressions)\n",
    "  document = document.translate(str.maketrans('', '', string.punctuation)) # remove the puntuations (, . ! # ...)\n",
    "  document = document.strip() # remove the spaces at the start and end of the headline\n",
    "  return [wnl.lemmatize(token) for token in word_tokenize(document) if token not in stopwords.words('english')]\n",
    "\n",
    "# The preprocess pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)), # passing custom tokenizer method for the CountVectorizer to use\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "\n",
    "tfidf_dataset = preprocessor.fit_transform(dataframe[\"TITLE\"].values) # process the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Models\n",
    "### 3.1 Decision Tree Classifier\n",
    "#### 3.1.1 Splitting the Dataset\n",
    "- The dataset is split into training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode the category labels\n",
    "le = LabelEncoder()\n",
    "class_label = le.fit_transform(dataframe[\"CATEGORY\"])\n",
    "\n",
    "# Split the data set into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_dataset.toarray(),\n",
    "    class_label,\n",
    "    test_size = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Decision Tree Classifier\n",
    "- A Decision Tree Classifier is trained on the TF-IDF transformed data.\n",
    "- The accuracy of the model is evaluated on the testing set.\n",
    "- An example prediction is demonstrated using a sample input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "#Decision Tree\n",
    "DTClass = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", random_state=40)\n",
    "DTClass.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = DTClass.predict(X_test)\n",
    "\n",
    "print(\"accuracy score of Decision Tree:\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Predicting the category of a new headline\n",
    "predicted = DTClass.predict(preprocessor.transform([\"Google's Android boss announces SDK specifically geared toward wearables\"]).toarray())\n",
    "print(dataframe[\"CATEGORY\"].values[np.where(class_label == predicted)[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multinomial Naive Bayes Classifier\n",
    "#### 3.2.1 Splitting the Dataset\n",
    "- The dataset is split into training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Encode the category labels\n",
    "le = LabelEncoder()\n",
    "class_label = le.fit_transform(dataframe[\"CATEGORY\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_dataset.toarray(), class_label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Naive Bayes Classifier\n",
    "- A Multinomial Naive Bayes Classifier is trained on the Count Vectorized data.\n",
    "- The accuracy of the model is evaluated on the testing set.\n",
    "- An example prediction is demonstrated using a sample input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = naive_bayes_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score of Naive Bayes:\")\n",
    "print(accuracy)\n",
    "\n",
    "predicted_category = naive_bayes_model.predict(preprocessor.transform([\"Google's Android boss announces SDK specifically geared toward wearables\"]).toarray())\n",
    "print(dataframe[\"CATEGORY\"].values[np.where(class_label == predicted_category)[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Neural Network Training\n",
    "- An Artificial Neural Network (ANN) is configured and trained on the dataset.\n",
    "- The accuracy of the model is evaluated on the testing set.\n",
    "- An example prediction is demonstrated using a sample input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(dataframe['TITLE'])\n",
    "sequences = tokenizer.texts_to_sequences(dataframe['TITLE'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = max(len(x) for x in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(dataframe['CATEGORY'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=1, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=16, input_length=max_length))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "# Function to preprocess and predict the category for a new title\n",
    "def predict_category(title, tokenizer, model, label_encoder, max_length):\n",
    "    # Tokenize and pad the title\n",
    "    sequence = tokenizer.texts_to_sequences([title])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Predict the category\n",
    "    prediction = model.predict(padded)\n",
    "    predicted_label_index = prediction.argmax()\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_label_index])\n",
    "    \n",
    "    return predicted_label[0]\n",
    "\n",
    "# Example usage\n",
    "title_to_test = \"Tina Fey named 'Mean Girls' character Glen Coco after a real person. Here's what he thinks about it, nearly 20 years later.\"\n",
    "predicted_category = predict_category(title_to_test, tokenizer, model, label_encoder, max_length)\n",
    "print(f\"The predicted category for the title '{title_to_test}' is '{predicted_category}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "- The script successfully collects and preprocesses the text data, encodes labels, and trains three classification models: Decision Tree, Naive Bayes, and Artificial Neural Network.\n",
    "- The accuracy scores and classification reports provide insights into the performance of each model on the testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

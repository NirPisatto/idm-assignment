{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDM:Assignment\n",
    "\n",
    "## News Classification based on Their Head lines\n",
    "\n",
    "### ChanpisethChap\n",
    "\n",
    "Royal University of PhnomPenh\n",
    "\n",
    "Group: MISA Pisatto, Hok Lenghak, Yorn Chanvisal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "### 1.1 Scraping News Articles\n",
    "\n",
    "Certainly! Below is the additional section for data collection in the report:\n",
    "\n",
    "## 1. Data Collection\n",
    "### 1.1 Scraping News Articles\n",
    "\n",
    "In the initial phase of the data collection process, the script employs Selenium and the Chrome WebDriver to systematically scrape news articles from the CNN website. The retrieved data is subsequently stored in a CSV file named 'data_set.csv'. To ensure data integrity, the script first checks for the existence of the CSV file; if absent, it creates the file with a header. The Chrome WebDriver is configured to navigate to a specific webpage, namely, 'https://www.nbcnews.com/archive/articles/2023/december', where news articles are archived. Following this, the script identifies the main HTML tag ('MonthPage') and extracts relevant information from anchor tags ('a') assumed to contain article titles. Each title, along with its corresponding ID and URL, is appended to a list for further processing.\n",
    "\n",
    "Subsequently, the script iterates through the collected titles, visiting each article's URL to extract additional details such as category, timestamp, author, and hostname. To maintain transparency and diagnose potential issues, the script prints the collected data and logs it into a file named 'logs.log'. Any encountered errors during the scraping process are caught, printed, and logged, ensuring a comprehensive record of the data collection process. Finally, after completing the scraping and data appending procedures, the Chrome WebDriver is gracefully closed, and a confirmation message is displayed, indicating the successful addition of data to the 'data_set.csv' file.\n",
    "\n",
    "We eexecuted this process a total of 12 times, spanning a duration of 12 months, with the objective of accumulating an extensive dataset of over 10,000 rows of news articles.\n",
    "\n",
    "After completing the aforementioned process, the resulting dataset includes the following entry:\n",
    "\n",
    "```\n",
    "'ID', 'TITLE', 'URL', 'Source', 'CATEGORY', 'Key', 'Website', 'Timestamp'\n",
    "1, \"Michael Latt, Hollywood social justice advocate, fatally shot in his home by an intruder, police say\", https://www.nbcnews.com/news/us-news/hollywood-social-justice-advocate-killed-home-intruder-rcna127406, U.S. NEWS, 2023-12-01T00:25:52.224Z, By Natalie Kainz and Phil Helsel, www.nbcnews.com\n",
    "```\n",
    "\n",
    "This entry signifies a news article titled \"Michael Latt, Hollywood social justice advocate, fatally shot in his home by an intruder, police say,\" categorized under U.S. NEWS. The article was published on December 1, 2023, at 00:25:52.224 UTC. It was authored by Natalie Kainz and Phil Helsel and is accessible via the URL: [https://www.nbcnews.com/news/us-news/hollywood-social-justice-advocate-killed-home-intruder-rcna127406](https://www.nbcnews.com/news/us-news/hollywood-social-justice-advocate-killed-home-intruder-rcna127406)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Declare and use logger incase app crashes scv won't be created\n",
    "logging.basicConfig(filename='logs.log', level=logging.INFO, format='%(asctime)s [%(levelname)s]: %(message)s')\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = 'data_set.csv'\n",
    "\n",
    "# Check if the file exists, if not create it with header\n",
    "file_exists = False\n",
    "try:\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        if any(reader):\n",
    "            file_exists = True\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set up the web driver (you need to have the appropriate web driver executable installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the NBC news 2023 december archive\n",
    "driver.get(\"https://www.nbcnews.com/archive/articles/2023/december\")\n",
    "\n",
    "# Get all the titles from the page\n",
    "main_tag = driver.find_element(By.CLASS_NAME, 'MonthPage')\n",
    "title_tags = main_tag.find_elements(By.TAG_NAME, 'a')\n",
    "titles = []\n",
    "index = 0\n",
    "\n",
    "# Loop through the titles and append them to the titles list\n",
    "for title in title_tags:\n",
    "    index += 1\n",
    "    title = {\"id\": index, \"title\": title.text, \"url\": title.get_attribute('href')}\n",
    "    titles.append(title)\n",
    "\n",
    "# Append data to the CSV file\n",
    "with open(csv_file_path, 'a', newline='') as file:\n",
    "\n",
    "    # Loop through the titles and get the category, timestamp, author and hostname\n",
    "    for title in titles:\n",
    "        try:\n",
    "            driver.get(title.get('url'))\n",
    "            span_tag = driver.find_element(By.CSS_SELECTOR,  'span[data-testid=\"unibrow-text\"]')\n",
    "            data_tag = driver.find_element(By.CSS_SELECTOR,  'time[data-testid=\"timestamp__datePublished\"]')\n",
    "            div_tag = driver.find_element(By.CSS_SELECTOR,  'div[data-activity-map=\"inline-byline-article-top\"]')\n",
    "            title['category'] = span_tag.text\n",
    "            title['timestamp'] = data_tag.get_attribute('content')\n",
    "            title['author'] = div_tag.text\n",
    "            title['hostname'] = 'www.nbcnews.com'\n",
    "\n",
    "            fieldnames = title.keys()\n",
    "            logging.info(f\"Appended to {csv_file_path}: {title}\")\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writerow(title)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print(f\"Data has been appended to {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "### 2.1 Loading Data\n",
    "- The script then loads the collected data from the CSV file using the pandas library.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11861, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data set file path\n",
    "src_file = 'all.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "dataframe = pd.read_csv(src_file, quotechar=\"\\\"\", engine='python', usecols=[\"TITLE\", \"CATEGORY\"])\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Filtering\n",
    "- The dataset is filtered based on the number of occurrences of each category.\n",
    "- Categories with counts less than or equal to 10 are excluded from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CATEGORY  COUNT\n",
      "0     #METOO RECKONING     12\n",
      "1        2022 ELECTION     81\n",
      "2        2024 ELECTION    165\n",
      "3      ABORTION RIGHTS    123\n",
      "4   AFTER GEORGE FLOYD     16\n",
      "..                 ...    ...\n",
      "87         U.K. ROYALS     28\n",
      "88      WAR IN UKRAINE    509\n",
      "89             WEATHER     55\n",
      "90   WESTERN WILDFIRES     19\n",
      "91         WHITE HOUSE    202\n",
      "\n",
      "[92 rows x 2 columns]\n",
      "                                               TITLE         CATEGORY\n",
      "0  A college professor called the police on two s...  CULTURE MATTERS\n",
      "1  Oscars producer says police were prepared to a...        CELEBRITY\n",
      "2  Jared Kushner interviewed by Jan. 6 committee ...     DONALD TRUMP\n",
      "3  House passes bill to cap out-of-pocket insulin...         CONGRESS\n",
      "5  Senate negotiators reach 'agreement in princip...         CONGRESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7698, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the data set to only the categories with more than 10 data points\n",
    "category_count = dataframe.groupby(\"CATEGORY\").size().reset_index(name='COUNT')\n",
    "category_count = category_count[category_count[\"COUNT\"] > 10]\n",
    "dataframe = dataframe[dataframe[\"CATEGORY\"].isin(category_count[\"CATEGORY\"])]\n",
    "\n",
    "\n",
    "# Remove the categories that are not needed\n",
    "dataframe = dataframe[~dataframe[\"CATEGORY\"].isin([\"U.S. NEWS\", \"NEWS\", \"WORLD\"])]\n",
    "\n",
    "\n",
    "print(dataframe.groupby(\"CATEGORY\").size().reset_index(name='COUNT'))\n",
    "print(dataframe.head())\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Handling Missing Data\n",
    "- The script checks for missing data and reports if any are found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO missing data\n"
     ]
    }
   ],
   "source": [
    "#check for missing data\n",
    "if(any(dataframe.isnull().any())):\n",
    "    print('Missing Data\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "else:\n",
    "    print('NO missing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Handling Duplicate Data\n",
    "- Duplicate rows are identified and removed from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found\n",
      "Number of duplicate rows=  40\n",
      "Dropping duplicates\n",
      "\n",
      "(7658, 2)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate\n",
    "if(any(dataframe.duplicated())==True):\n",
    "    print('Duplicate rows found')\n",
    "    print('Number of duplicate rows= ', dataframe[dataframe.duplicated()].shape[0])\n",
    "    dataframe.drop_duplicates(inplace=True,keep='first')\n",
    "    dataframe.reset_index(inplace=True,drop=True)\n",
    "    print('Dropping duplicates\\n')\n",
    "    print(dataframe.shape)\n",
    "else:\n",
    "    print('NO duplicate data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Text Cleaning and Tokenization\n",
    "- NLTK libraries are used to download necessary resources (stopwords, punkt, wordnet).\n",
    "- A custom tokenizer function is defined to lowercase text, remove digits, punctuation, and stopwords, and lemmatize the remaining tokens.\n",
    "- A pipeline is created to perform Count Vectorization and TF-IDF transformation on the 'TITLE' column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/misapisatto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/misapisatto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/misapisatto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/misapisatto/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Set the sklearn pipeline to return pandas dataframe\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Function for cleaning and tokenize the headline\n",
    "def tokenize(doc):\n",
    "  document = doc.lower() # convert the content of the headline to lowercase\n",
    "  document = re.sub(r'\\d+', '', document) # remove all of the digits inside of the content (using regular expressions)\n",
    "  document = document.translate(str.maketrans('', '', string.punctuation)) # remove the puntuations (, . ! # ...)\n",
    "  document = document.strip() # remove the spaces at the start and end of the headline\n",
    "  return [wnl.lemmatize(token) for token in word_tokenize(document) if token not in stopwords.words('english')]\n",
    "  # tokenize the headlines\n",
    "  # and then filter only the words that are not in the english stopwords (words that are commonly used and give no benifits to the classifier)\n",
    "  # and finally lemmatize all of the tokens\n",
    "\n",
    "# The preprocess pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)), # passing custom tokenizer method for the CountVectorizer to use\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "\n",
    "tfidf_dataset = preprocessor.fit_transform(dataframe[\"TITLE\"].values) # process the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Models\n",
    "### 3.1 Decision Tree Classifier\n",
    "#### 3.1.1 Splitting the Dataset\n",
    "- The dataset is split into training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode the category labels\n",
    "le = LabelEncoder()\n",
    "class_label = le.fit_transform(dataframe[\"CATEGORY\"])\n",
    "\n",
    "# Split the data set into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_dataset.toarray(),\n",
    "    class_label,\n",
    "    test_size = 0.3 # the size of the testing dataset (in percentage between 0 and 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Decision Tree Classifier\n",
    "- A Decision Tree Classifier is trained on the TF-IDF transformed data.\n",
    "- The accuracy of the model is evaluated on the testing set.\n",
    "- An example prediction is demonstrated using a sample input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41]\n",
      "IMMIGRATION\n",
      "accuracy score of Decision Tree:\n",
      "0.4225413402959095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "#Decision Tree\n",
    "DTClass = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", random_state=40)\n",
    "DTClass.fit(X_train, y_train)\n",
    "y_pred = DTClass.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"accuracy score of Decision Tree:\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Predicting the category of a new headline\n",
    "predicted = DTClass.predict(preprocessor.transform([\"Slashing Central American aid could drive more migrants to the U.S.\"]).toarray())\n",
    "print(o)\n",
    "print(dataframe[\"CATEGORY\"].values[np.where(class_label == predicted)[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multinomial Naive Bayes Classifier\n",
    "#### 3.2.1 Splitting the Dataset\n",
    "- The dataset is split into training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "naive_bayes_model = MultinomialNB()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataframe['TITLE'], dataframe['CATEGORY'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Naive Bayes Classifier\n",
    "- A Multinomial Naive Bayes Classifier is trained on the Count Vectorized data.\n",
    "- The accuracy of the model is evaluated on the testing set.\n",
    "- An example prediction is demonstrated using a sample input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Naive Bayes:\n",
      "Predicted Category: CULTURE MATTERS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/misapisatto/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/misapisatto/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/misapisatto/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "naive_bayes_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = naive_bayes_model.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score of Naive Bayes:\")\n",
    "\n",
    "new_title_vectorized = vectorizer.transform([\"At least five states are considering requiring full minimum wages for tip earners this year\"])\n",
    "\n",
    "predicted_category = naive_bayes_model.predict(new_title_vectorized)\n",
    "\n",
    "print(f'Predicted Category: {predicted_category[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Artificial Neural Network\n",
    "#### 3.3.1 Splitting the Dataset\n",
    "- The dataset is split into training and testing sets.\n",
    "\n",
    "#### 3.3.2 Neural Network Training\n",
    "- An Artificial Neural Network (ANN) is configured and trained on the dataset.\n",
    "- The accuracy of the model is evaluated on the testing set.\n",
    "- An example prediction is demonstrated using a sample input.\n",
    "\n",
    "## 4. Conclusion\n",
    "- The script successfully collects and preprocesses the text data, encodes labels, and trains three classification models: Decision Tree, Naive Bayes, and Artificial Neural Network.\n",
    "- The accuracy scores and classification reports provide insights into the performance of each model on the testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
